
# ğŸ§  Real-time Indian News AI Chatbot

> A full-stack AI-powered application that delivers real-time answers to user queries based on the latest Indian news â€” using Pathway, Ollama, and Streamlit. Supports topic alerts, domain-based filtering, and a modern chat interface.

---

## ğŸ“Œ Features

âœ… Real-time Retrieval-Augmented Generation (RAG)  
âœ… Indian news via Exa API  
âœ… Ollama-powered local LLM (LLaMA3)  
âœ… User-friendly AI chatbot interface (Streamlit)  
âœ… Topic-based notification alerts  
âœ… Domain-based filtering (sports, politics, tech, etc.)

---

## ğŸ§± Tech Stack

| Layer      | Technology         |
|------------|--------------------|
| Backend    | Python + FastAPI + Pathway |
| Embeddings | sentence-transformers (local) |
| LLM        | Ollama (`llama3`, `mistral`, etc.) |
| Frontend   | Streamlit (chat-style UI) |
| Data Feed  | Exa API (for latest Indian news) |

---

## ğŸ“¦ Clone & Setup

### ğŸ”§ Prerequisites

- WSL 2 (Linux Subsystem on Windows) or native Linux/macOS
- Python 3.10+
- [Ollama installed](https://ollama.com)
- `llama3` or similar model pulled via Ollama

---

### ğŸ“ Clone the Repository

```bash
git clone https://github.com/your-username/realtime-rag-bot.git
cd realtime-rag-bot
```

---

## âš™ï¸ Backend Setup (WSL or Linux)

```bash
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install backend dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

### ğŸ“„ Create `.env` file

```env
EXA_API_KEY=your_exa_api_key_here
```

> Get your key from: [https://exa.ai](https://exa.ai)

---

### â–¶ï¸ Run Ollama

Make sure Ollama is installed and running:

```bash
ollama run llama3
```

> First time? This will download the model (~4â€“7GB)

---

### â–¶ï¸ Run FastAPI Backend

```bash
uvicorn backend.api.app:app --reload --port 8000
```

Open: [http://localhost:8000/docs](http://localhost:8000/docs)  
to test `/ask` and `/subscribe` endpoints.

---

## ğŸ’¬ Frontend (Streamlit)

In a new terminal:

```bash
cd frontend
source ../.venv/bin/activate
pip install -r requirements.txt
streamlit run app.py
```

Open in browser: [http://localhost:8501](http://localhost:8501)

---

## ğŸ’» Usage

### ğŸ§  Ask AI Questions

- Enter your question (e.g., â€œWhatâ€™s new in Indian elections?â€)
- Select a domain (e.g., politics, sports)
- Get an answer generated using real-time indexed news

### ğŸ”” Subscribe to Topic Alerts

- Enter a keyword (e.g., â€œcricketâ€)
- Get notified (in logs or UI) when relevant news is ingested

---

## ğŸ§ª Testing the Flow

1. Run Ollama
2. Start backend
3. Run Streamlit
4. Subscribe to a topic (e.g., "ipl")
5. Ask a domain-specific question (e.g., "Any updates in IPL?")

---

## ğŸ“ Project Structure

```
realtime-rag-bot/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ingestion/             # Exa news fetch + stream
â”‚   â”œâ”€â”€ pathway_pipeline/      # Real-time ETL using Pathway
â”‚   â”œâ”€â”€ retrieval/             # Vector search + prompt creation
â”‚   â”œâ”€â”€ llm/                   # Ollama LLM client
â”‚   â”œâ”€â”€ api/                   # FastAPI endpoints
â”‚   â””â”€â”€ config/                # Subscriptions store
â”‚
â”œâ”€â”€ frontend/                  # Streamlit chat UI
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ .env                       # EXA API key
â”œâ”€â”€ requirements.txt           # Backend deps
â””â”€â”€ README.md
```

---

## ğŸ”§ Dev Tips

- Customize LLM: Change `MODEL_NAME` in `ollama_client.py`
- Add more domains: Extend `domain` choices in Streamlit
- Add email/Slack alerts: Replace `print()` with webhook/email logic
- For production: Add Docker support or deploy on Hugging Face Spaces

---

## ğŸ¤ Credits

Built for the **Pathway Real-time RAG Playground Hackathon**  
Tech powered by:
- [Pathway](https://pathway.com/)
- [Ollama](https://ollama.com/)
- [Streamlit](https://streamlit.io/)
- [Exa API](https://exa.ai/)

---

## ğŸªª License

MIT License. See `LICENSE` file.
